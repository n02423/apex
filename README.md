# SoilVision - AI-Powered Soil Classification App

SoilVision is an innovative iOS application that enables farmers, researchers, and agronomists to instantly identify soil types using artificial intelligence. Simply capture or upload a soil image, and the app will classify it as clay, loam, sandy, silt, peat, or chalk with confidence scores.

## ğŸŒŸ Features

### Core Functionality
- **Instant Soil Classification**: Classify soil samples into 6 types (clay, loam, sandy, silt, peat, chalk)
- **Camera Integration**: Native iOS camera with real-time preview and guidance
- **Image Processing**: Advanced preprocessing for optimal ML model performance
- **Confidence Scoring**: Detailed confidence metrics and probability breakdowns
- **Offline-First**: Works without internet connection with local SQLite storage
- **Location Tagging**: Optional GPS coordinates for analysis mapping
- **Export & Sharing**: PDF reports, CSV data, and social media sharing

### User Experience
- **Onboarding**: Smooth introduction with optional authentication
- **Guest Mode**: Full functionality without account registration
- **History Tracking**: Complete scan history with search and filter capabilities
- **Modern UI**: Clean, intuitive interface with earth-tone design system
- **Accessibility**: VoiceOver support and high contrast options

## ğŸ—ï¸ Architecture

### Technology Stack
- **Frontend**: SwiftUI with MVVM architecture
- **Machine Learning**: CoreML with Vision framework
- **Local Storage**: SQLite for offline data persistence
- **Cloud Services**: Firebase Auth and Firestore (optional)
- **Camera**: AVFoundation for native camera integration
- **Location**: CoreLocation for GPS tagging

### Project Structure
```
SoilVision/
â”œâ”€â”€ App/                              # iOS Application
â”‚   â”œâ”€â”€ SoilVisionApp.swift          # App entry point
â”‚   â”œâ”€â”€ ContentView.swift            # Root view controller
â”‚   â”œâ”€â”€ Views/                       # SwiftUI Views
â”‚   â”‚   â”œâ”€â”€ CameraView.swift         # Camera interface
â”‚   â”‚   â”œâ”€â”€ ResultView.swift         # Classification results
â”‚   â”‚   â”œâ”€â”€ HistoryView.swift        # Test history
â”‚   â”‚   â”œâ”€â”€ ProfileView.swift        # User profile
â”‚   â”‚   â””â”€â”€ OnboardingView.swift     # Welcome screens
â”‚   â”œâ”€â”€ Models/                      # Data Models
â”‚   â”‚   â”œâ”€â”€ SoilResult.swift         # Soil test result
â”‚   â”‚   â”œâ”€â”€ User.swift               # User profile
â”‚   â”‚   â””â”€â”€ LocationData.swift       # GPS location
â”‚   â”œâ”€â”€ ViewModels/                  # MVVM View Models
â”‚   â”‚   â”œâ”€â”€ CameraViewModel.swift    # Camera logic
â”‚   â”‚   â”œâ”€â”€ ResultViewModel.swift    # Result processing
â”‚   â”‚   â”œâ”€â”€ HistoryViewModel.swift   # History management
â”‚   â”‚   â””â”€â”€ ProfileViewModel.swift   # User profile logic
â”‚   â”œâ”€â”€ Services/                    # Business Logic
â”‚   â”‚   â”œâ”€â”€ AuthManager.swift        # Firebase authentication
â”‚   â”‚   â”œâ”€â”€ DatabaseManager.swift    # SQLite operations
â”‚   â”‚   â”œâ”€â”€ SoilClassifierService.swift  # ML inference
â”‚   â”‚   â”œâ”€â”€ ImageProcessor.swift     # Image preprocessing
â”‚   â”‚   â”œâ”€â”€ LocationManager.swift    # GPS services
â”‚   â”‚   â”œâ”€â”€ SyncManager.swift        # Cloud sync
â”‚   â”‚   â””â”€â”€ ReportExporter.swift     # Export functionality
â”‚   â”œâ”€â”€ Utils/                       # Helper Utilities
â”‚   â”‚   â”œâ”€â”€ Extensions.swift         # Swift extensions
â”‚   â”‚   â”œâ”€â”€ Constants.swift          # App constants
â”‚   â”‚   â””â”€â”€ Validators.swift         # Input validation
â”‚   â”œâ”€â”€ Resources/                   # External Resources
â”‚   â”‚   â””â”€â”€ SoilClassifier.mlmodel   # Trained ML model
â”‚   â””â”€â”€ SoilVision.xcodeproj         # Xcode project file
â”œâ”€â”€ ML_Model/                        # Machine Learning Components
â”‚   â”œâ”€â”€ train_model.py              # Training script
â”‚   â”œâ”€â”€ convert_to_coreml.py        # Model conversion
â”‚   â”œâ”€â”€ dataset/                    # Training images
â”‚   â”‚   â”œâ”€â”€ clay/                   # Clay soil images
â”‚   â”‚   â”œâ”€â”€ loam/                   # Loam soil images
â”‚   â”‚   â”œâ”€â”€ sandy/                  # Sandy soil images
â”‚   â”‚   â”œâ”€â”€ silt/                   # Silt soil images
â”‚   â”‚   â”œâ”€â”€ peat/                   # Peat soil images
â”‚   â”‚   â””â”€â”€ chalk/                  # Chalk soil images
â”‚   â””â”€â”€ model_outputs/              # Trained models
â””â”€â”€ Documentation/                   # Project documentation
```

## ğŸš€ Getting Started

### Prerequisites
- **Xcode 15+** with iOS 16.0+ SDK
- **Swift 5.8+**
- **Physical iOS device** (for camera testing)
- **Python 3.8+** (for ML model training)
- **TensorFlow 2.x** (for model training)
- **coremltools** (for model conversion)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/soilvision.git
   cd soilvision
   ```

2. **Open in Xcode**
   ```bash
   open SoilVision/App/SoilVision.xcodeproj
   ```

3. **Configure Firebase (Optional)**
   - Create a Firebase project at https://console.firebase.google.com
   - Download `GoogleService-Info.plist` and add to the project
   - Enable Authentication and Firestore services

4. **Build and Run**
   - Select your physical device (camera required)
   - Press `Cmd+R` to build and run the app

### ML Model Training (Optional)

If you want to train your own soil classification model:

1. **Prepare Dataset**
   ```bash
   mkdir -p ML_Model/dataset/{clay,loam,sandy,silt,peat,chalk}
   # Add at least 500 images per class
   ```

2. **Install Python Dependencies**
   ```bash
   pip install tensorflow opencv-python numpy matplotlib scikit-learn
   pip install coremltools  # For iOS conversion
   ```

3. **Train Model**
   ```bash
   cd ML_Model
   python3 train_model.py --data_dir ./dataset --output_dir ./model_outputs
   ```

4. **Convert to CoreML**
   ```bash
   python3 convert_to_coreml.py --model_path ./model_outputs/soil_classifier_model.h5
   ```

5. **Add to iOS Project**
   - Copy `SoilClassifier.mlmodel` to `App/Resources/`
   - Add to Xcode project target

## ğŸ“± Usage Guide

### First Launch
1. **Onboarding**: Swipe through introduction screens
2. **Authentication**: Choose guest mode or create an account
3. **Permissions**: Grant camera and optional location permissions

### Analyzing Soil
1. **Capture Photo**: Use the built-in camera or select from gallery
2. **Position Sample**: Center soil in the camera frame
3. **Review & Analyze**: Confirm image and tap "Analyze Soil"
4. **View Results**: See classification with confidence scores
5. **Save or Share**: Save to history or export results

## ğŸ¤– Machine Learning

### Model Architecture
```
Input (224x224x3)
â”œâ”€â”€ Conv2D(32, 3x3, ReLU) + BatchNorm
â”œâ”€â”€ Conv2D(32, 3x3, ReLU) + BatchNorm
â”œâ”€â”€ MaxPooling2D + Dropout(0.25)
â”œâ”€â”€ Conv2D(64, 3x3, ReLU) + BatchNorm
â”œâ”€â”€ Conv2D(64, 3x3, ReLU) + BatchNorm
â”œâ”€â”€ MaxPooling2D + Dropout(0.25)
â”œâ”€â”€ Conv2D(128, 3x3, ReLU) + BatchNorm
â”œâ”€â”€ Conv2D(128, 3x3, ReLU) + BatchNorm
â”œâ”€â”€ MaxPooling2D + Dropout(0.25)
â”œâ”€â”€ Conv2D(256, 3x3, ReLU) + BatchNorm
â”œâ”€â”€ Conv2D(256, 3x3, ReLU) + BatchNorm
â”œâ”€â”€ MaxPooling2D + Dropout(0.25)
â”œâ”€â”€ Flatten
â”œâ”€â”€ Dense(512, ReLU) + BatchNorm + Dropout(0.5)
â”œâ”€â”€ Dense(256, ReLU) + BatchNorm + Dropout(0.4)
â””â”€â”€ Dense(6, Softmax)
```

### Training Parameters
- **Optimizer**: Adam (lr=0.001)
- **Loss**: Categorical Crossentropy
- **Metrics**: Accuracy, Top-K Accuracy
- **Batch Size**: 32
- **Epochs**: 50 (with early stopping)
- **Data Augmentation**: Rotation, zoom, brightness, contrast

## ğŸ”§ Configuration

### App Settings
Edit `SoilVision/App/Utils/Constants.swift` for custom configuration:

```swift
struct AppConstants {
    static let appName = "SoilVision"
    static let appVersion = "1.0"

    struct ML {
        static let confidenceThreshold: Double = 0.6
        static let imageSize = CGSize(width: 224, height: 224)
    }
}
```

## ğŸ§ª Testing

### Unit Tests
```bash
# Run unit tests
xcodebuild test -scheme SoilVision -destination 'platform=iOS Simulator,name=iPhone 14'
```

### UI Tests
- Camera functionality (requires physical device)
- Navigation and user flows
- Accessibility testing

## ğŸ“Š Performance

### Benchmarks
- **App Launch**: <3 seconds
- **Camera Capture**: <500ms latency
- **Image Processing**: <1 second
- **ML Inference**: <2 seconds
- **Memory Usage**: <150MB peak

## ğŸ”’ Security & Privacy

### Data Protection
- **On-device Processing**: All ML inference happens locally
- **Local Encryption**: SQLite database encryption
- **No Tracking**: No analytics or tracking
- **Privacy by Design**: Minimal data collection

## ğŸ“„ License

This project is licensed under the MIT License.

---

**SoilVision** - Making soil science accessible to everyone, one photo at a time. ğŸŒ±